# ArxisVR - Ollama AI Integration Guide

## üìã Pr√©-requisitos

1. **Instalar Ollama**
   - Windows: Baixe em https://ollama.ai/download
   - Ou use: `winget install Ollama.Ollama`

2. **Baixar o modelo recomendado**
   ```bash
   ollama pull llama3.2:3b
   ```

   **Modelos alternativos** (caso sua m√°quina aguente mais):
   ```bash
   # Modelos de texto menores (recomendado para m√°quinas com 8-16GB RAM)
   ollama pull llama3.2:3b      # ~2GB - RECOMENDADO
   ollama pull phi3:mini         # ~2.3GB - R√°pido
   
   # Modelos maiores (16GB+ RAM)
   ollama pull llama3:8b         # ~4.7GB
   ollama pull mistral:7b        # ~4GB
   
   # Modelos especializados
   ollama pull codellama:7b      # Para c√≥digo
   ollama pull llama3-uncensored # Sem filtros
   ```

3. **Verificar se o Ollama est√° rodando**
   ```bash
   ollama list
   ```

## üöÄ Como Usar

### 1. Iniciar o Ollama
O Ollama deve iniciar automaticamente ap√≥s a instala√ß√£o. Caso contr√°rio:
```bash
ollama serve
```

### 2. Configurar o ambiente
O arquivo `.env` j√° est√° configurado com:
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

### 3. Usar no c√≥digo

```csharp
using ArxisVR.AI;

// Carregar configura√ß√£o
var config = AIConfig.LoadFromEnvironment();

// Criar servi√ßo Ollama
using var ollama = new OllamaService(config);

// Verificar disponibilidade
if (await ollama.IsAvailableAsync())
{
    Console.WriteLine("‚úÖ Ollama est√° dispon√≠vel!");
    
    // Listar modelos
    var models = await ollama.GetAvailableModelsAsync();
    Console.WriteLine($"Modelos dispon√≠veis: {string.Join(", ", models)}");
}

// Criar assistente especializado em IFC
var assistant = new IfcAIAssistant(ollama);

// Fazer perguntas
var response = await assistant.AskAsync("Como fa√ßo para medir dist√¢ncias no modelo?");
Console.WriteLine(response);

// Analisar elementos IFC
var properties = new Dictionary<string, string>
{
    ["Type"] = "IfcWall",
    ["Height"] = "3.0m",
    ["Thickness"] = "0.2m",
    ["Material"] = "Concrete"
};
var analysis = await assistant.AnalyzeElementAsync("Wall", properties);
Console.WriteLine(analysis);
```

## üéØ Funcionalidades

### OllamaService
- ‚úÖ Verificar disponibilidade do servi√ßo
- ‚úÖ Listar modelos instalados
- ‚úÖ Gerar respostas simples
- ‚úÖ Gerar respostas em streaming
- ‚úÖ Chat com contexto (mem√≥ria de conversa)

### IfcAIAssistant
- ‚úÖ Assistente especializado em IFC/BIM
- ‚úÖ An√°lise de elementos e propriedades
- ‚úÖ Ajuda contextual com recursos
- ‚úÖ Sugest√µes inteligentes
- ‚úÖ Hist√≥rico de conversa√ß√£o

## üí° Exemplos de Uso

### Chat simples
```csharp
var assistant = new IfcAIAssistant(ollama);
var answer = await assistant.AskAsync("O que √© um arquivo IFC?");
```

### Streaming (para respostas em tempo real)
```csharp
await foreach (var chunk in ollama.GenerateStreamAsync("Explique BIM"))
{
    Console.Write(chunk);
}
```

### Obter ajuda sobre recursos
```csharp
var help = await assistant.GetFeatureHelpAsync("VR Mode");
```

### Sugest√µes contextuais
```csharp
var suggestions = await assistant.GetSuggestionsAsync("Usu√°rio abriu um modelo IFC grande");
// Retorna: ["Ative o modo de camadas para melhor performance", ...]
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

No `AIConfig`, voc√™ pode ajustar:

```csharp
var config = new AIConfig
{
    OllamaBaseUrl = "http://localhost:11434",
    OllamaModel = "llama3.2:3b",
    MaxTokens = 2048,          // Tamanho m√°ximo da resposta
    Temperature = 0.7f         // 0.0 = conservador, 1.0 = criativo
};
```

## üîß Solu√ß√£o de Problemas

### Ollama n√£o conecta
```bash
# Verificar se est√° rodando
curl http://localhost:11434/api/tags

# Reiniciar servi√ßo
ollama serve
```

### Modelo n√£o encontrado
```bash
# Verificar modelos instalados
ollama list

# Baixar modelo
ollama pull llama3.2:3b
```

### Mem√≥ria insuficiente
- Use modelos menores: `phi3:mini` ou `llama3.2:3b`
- Reduza `MaxTokens` no `AIConfig`
- Feche outros programas

## üìä Requisitos de Sistema

| Modelo | RAM m√≠nima | Tamanho | Velocidade |
|--------|-----------|---------|------------|
| llama3.2:3b | 8GB | ~2GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| phi3:mini | 8GB | ~2.3GB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| llama3:8b | 16GB | ~4.7GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| mistral:7b | 16GB | ~4GB | ‚≠ê‚≠ê‚≠ê‚≠ê |
| llama3-uncensored | 16GB | ~4.7GB | ‚≠ê‚≠ê‚≠ê‚≠ê |

## üîí Seguran√ßa

- ‚úÖ `.env` est√° no `.gitignore` (API keys n√£o vazam)
- ‚úÖ Ollama roda localmente (dados n√£o saem da m√°quina)
- ‚úÖ Sem telemetria ou tracking

## üìö Links √öteis

- [Ollama GitHub](https://github.com/ollama/ollama)
- [Ollama Models Library](https://ollama.ai/library)
- [API Documentation](https://github.com/ollama/ollama/blob/main/docs/api.md)
